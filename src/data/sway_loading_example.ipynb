{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9878deb-2e5d-4b87-810b-4d59a12e9c93",
   "metadata": {},
   "source": [
    "## This notebook demonstrates how to correctly load and visualize the preprocessed sway data.\n",
    "```\n",
    "Dataset hierarchy is managed as follows:\n",
    "data/sway61769/dataset61769.txt                                 ----> a list of all the video folder names\n",
    "              /sway61769/video_folder_1/images/{frame:05}.jpg   ----> all image files\n",
    "                                       /intrinsics.npy\n",
    "                                       /extrinsics.npy\n",
    "                                       /bbox.npy\n",
    "                                       /wspace_poses3d.npy      ----> w-space 3d poses that are consistent with h36m coordinates\n",
    "                                       /cspace-poses3d.npy      ----> c-space 3d poses that are consistent with h36m coordinates \"in root-relative space\"\n",
    "                                       /keypts2d.json\n",
    "              /sway61769/video_folder_2\n",
    "              /sway61769/video_folder_3\n",
    "              /sway61769/video_folder_4\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2feb159-aa1a-45a6-be5d-aee0ea952c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "#import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from skimage.io import imread, imshow\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "from numpy.linalg import inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad88771a-953d-4760-bd46-525baef60e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "### IMPORTANT!!! Joint order information\n",
    "n_joints = 22\n",
    "smpljoint_order = [\"hips\",\n",
    "                   \"left_hip\",\n",
    "                   \"right_hip\",\n",
    "                   \"spine\",\n",
    "                   \"left_knee\",\n",
    "                   \"right_knee\",\n",
    "                   \"spine1\",\n",
    "                   \"left_ankle\",\n",
    "                   \"right_ankle\",\n",
    "                   \"spine2\",\n",
    "                   \"left_toe_base\",\n",
    "                   \"right_toe_base\",\n",
    "                   \"neck\",\n",
    "                   \"left_shoulder\",\n",
    "                   \"right_shoulder\",\n",
    "                   \"head\",\n",
    "                   \"left_upperarm\",\n",
    "                   \"right_upperarm\",\n",
    "                   \"left_elbow\",\n",
    "                   \"right_elbow\",\n",
    "                   \"left_wrist\",\n",
    "                   \"right_wrist\",\n",
    "                   ]\n",
    "\n",
    "### These are the joints I used to match the joint order of the h36m prediction output in my IK visualizer. You might need further confirmation when loading them to METRO\n",
    "# hip, head, r_upperarm, r_elbow, r_wrist, l_upperarm, l_elbow, l_wrist\n",
    "smpl_7joints_indices = [0, 15, 17, 19, 21, 16, 18, 20]\n",
    "\n",
    "\n",
    "### 2D keypoint order. It follows openpose format. You might need it later for 2Ddataset\n",
    "kpt2d_order = [\"nose\",\n",
    "               \"neck\",\n",
    "               \"r_shoulder\",\n",
    "               \"r_elbow\",\n",
    "               \"r_hand\",\n",
    "               \"l_shoulder\",\n",
    "               \"l_elbow\",\n",
    "               \"l_hand\",\n",
    "               \"pelvis\",\n",
    "               \"r_hips\",\n",
    "               \"r_knee\",\n",
    "               \"r_ankle\",\n",
    "               \"l_hips\",\n",
    "               \"l_knee\",\n",
    "               \"l_ankle\",\n",
    "               \"r_eye\",\n",
    "               \"l_eye\",\n",
    "               \"r_ear\",\n",
    "               \"l_ear\",\n",
    "               \"l_toe\",\n",
    "               \"l_toe_end\",\n",
    "               \"l_ankle_end\",\n",
    "               \"r_toe\",\n",
    "               \"r_toe_end\",\n",
    "               \"r_ankle_end\",\n",
    "               ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8fbe03-f0d8-469e-8437-8d7a75c0359d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sway_folder = \"/home/jovyan/data/sway61769\"\n",
    "root = f\"{sway_folder}/sway61769\"\n",
    "\n",
    "### Get all video folder names. Don't us os.listdir(root) because it is too slow\n",
    "with open(f\"{sway_folder}/dataset61769.txt\", \"r\") as f:\n",
    "    vidnames = [line.strip() for line in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe05ff3-6e36-4754-b8d5-c8874ec1f276",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vidnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cbbd642-e1ab-48e3-b970-7c24fd2844ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load all annotations from a video folder\n",
    "\n",
    "def load_all_annotations_from_folder(folderpath):\n",
    "    kptpath = os.path.join(folderpath, \"keypts2d.json\")\n",
    "    world3dpath = os.path.join(folderpath, \"wspace_poses3d.npy\")\n",
    "    cam3dpath = os.path.join(folderpath, \"cspace-poses3d.npy\")\n",
    "    camextpath = os.path.join(folderpath, \"extrinsics.npy\")\n",
    "    camintpath = os.path.join(folderpath, \"intrinsics.npy\")\n",
    "    bboxpath = os.path.join(folderpath, \"bbox.npy\")\n",
    "    \n",
    "    ## W-space 3D Poses\n",
    "    world_pose3d = np.load(world3dpath)\n",
    "    \n",
    "    ## C-space 3D Poses\n",
    "    cam_pose3d = np.load(cam3dpath)\n",
    "    \n",
    "    ## Camera intrinsics and extrinsics\n",
    "    intrinsics = np.load(camintpath)\n",
    "    extrinsics = np.load(camextpath)\n",
    "    \n",
    "    ## Bounding boxes\n",
    "    bbox = np.load(bboxpath)\n",
    "    \n",
    "    ## 2D keypoints\n",
    "    with open(kptpath, \"r\") as f:\n",
    "        kpt_params = json.load(f)\n",
    "    keypoints = kpt_params[\"key_points\"]\n",
    "\n",
    "    return world_pose3d, cam_pose3d, intrinsics, extrinsics, bbox, keypoints\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414ca2ab-8226-4d9c-bdb1-b0820bc076bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Randomly choose a video and do camera projection to visualize its overlay with image.\n",
    "np.random.seed(12)\n",
    "vidname = np.random.choice(vidnames)\n",
    "folderpath = os.path.join(root, vidname)\n",
    "print(vidname)\n",
    "world_pose3d, cam_pose3d, intrinsics, extrinsics, bbox, keypoints = load_all_annotations_from_folder(folderpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52dbd00b-fdeb-45c1-946e-b725740bc8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"W-space pose (#fr,22,3):\", world_pose3d.shape, \" C-space pose (#fr,22,3):\", cam_pose3d.shape, \n",
    "      \" Intrinsics (3,3):\", intrinsics.shape, \" Extrinsics (4,4):\", extrinsics.shape, \n",
    "      \" Bbox (#fr,4):\", bbox.shape)\n",
    "print(\"\\nPreview keypoint data:\\n\", keypoints['0']) \n",
    "print(\"\\nLength of 2D keypoints:\", len(keypoints))\n",
    "print(f\"intrinsics: {intrinsics}\\nextrinsics: {extrinsics}\\nworld_pose3d: {world_pose3d[0]})\n",
    "### Keypoints are represented in relative ratio\n",
    "### Bounding boxes are represented in (x, y, w, h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93daa14-291a-4f99-b4b4-f5ffe589501d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d08bd5c-65b4-46e2-ab96-ebbff925fd3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Camera projection from world space 3d poses\n",
    "\n",
    "# Pad 1 to make pose a 4d vector: (L, J, 3) --> (L, J, 4)\n",
    "world_pose4d = np.concatenate((world_pose3d, np.ones([world_pose3d.shape[0], world_pose3d.shape[1], 1])), axis=-1)\n",
    "\n",
    "# World to camera coordinate: (4, 4) x (L, J, 4) --> (L, J, 4)\n",
    "cam_pose4d = np.tensordot(extrinsics, world_pose4d, axes=([1], [2])).transpose(1, 2, 0)\n",
    "recalculated_cam_pose3d = cam_pose4d[..., :3] # (L, J, 3)\n",
    "print(\"diff:\", np.max(np.abs(cam_pose3d - recalculated_cam_pose3d))) ### The same as the saved cspace 3d pose\n",
    "        \n",
    "## homogeneous coordinates: (L, J, 3)\n",
    "cam_pose3d_homo = cam_pose3d / cam_pose3d[..., -1:]\n",
    "\n",
    "## project to image space: (2, 3) x (L, J, 3) --> (L, J, 2)\n",
    "joint2d_proj = np.tensordot(intrinsics[:2], cam_pose3d_homo, axes=([1], [2])).transpose(1, 2, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9edbc7af-2ce7-4952-83ef-55b7ed8abddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Obtain all image filenames\n",
    "imagefiles = [f for f in sorted(os.listdir(os.path.join(root, vidname, \"images\"))) if f.endswith(\".jpg\")]\n",
    "\n",
    "########## IMPORTANT!!! Note that image files may be 1~2 frames longer than pose sequences. Always use the length of pose sequence as reference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d1dea8-c65c-4ed2-8c51-7b44727f8c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def overlay_visualization(imagepath, keypoint_2d, projected_2d, bbox):\n",
    "    image = imread(imagepath)\n",
    "    h, w, _ = image.shape\n",
    "    plt.imshow(image)\n",
    "    \n",
    "    ### Show the reprojected keypoints\n",
    "    for j in range(projected_2d.shape[0]):\n",
    "        plt.plot(projected_2d[j, 0], projected_2d[j, 1], \"o\", markersize=7, color=\"orange\")\n",
    "        \n",
    "    ### Show the keypoints\n",
    "    for joint2d in keypoint_2d:\n",
    "        x = joint2d['u'] * w\n",
    "        y = joint2d['v'] * h\n",
    "        plt.plot(x, y, \"o\", markersize=3, color=\"white\", alpha=joint2d['confidence'])\n",
    "    min_x = bbox[0]\n",
    "    min_y = bbox[1]\n",
    "    max_x = bbox[0] + bbox[2]\n",
    "    max_y = bbox[1] + bbox[3]\n",
    "    plt.plot([min_x, max_x, max_x, min_x, min_x], [min_y, min_y, max_y, max_y, min_y])\n",
    "    plt.show() \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64231863-0be1-47b6-9f9d-7fc33ab1f534",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_frames = world_pose3d.shape[0]\n",
    "frame_index = np.random.randint(0, n_frames)\n",
    "\n",
    "imagepath = os.path.join(root, vidname, \"images\", imagefiles[frame_index])\n",
    "\n",
    "overlay_visualization(imagepath, keypoints[str(frame_index)], joint2d_proj[frame_index, smpl_7joints_indices], bbox[frame_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d916089-7383-4f8f-b737-f63332189eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Note that 2d keypoints and overlaid poses may be out of image frames because they are estimated. However, bbox would always be within image boundary."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
